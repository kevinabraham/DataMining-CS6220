---
title: "YELP Dataset Analysis"
date: "November 30, 2015"
output: pdf_document
---

# Trees

## Read & Clean
```{r}
# Read the data from CSV
yelp = read.csv("YelpUsers-V2.csv")

# Remove less useful columns from the dataset (country, name, Id)
yelp = yelp[, -c(1:3)]

# Check the data for missing values
sum(is.na(yelp))
```


## Extract only Elite users
```{r}
# Remove non-Elite members from dataset. 
yelpElite = yelp[(yelp$EliteYearCount > 0), ]
```

## Count years to become elite
```{r}
# Convert string to date
yelpElite$YelpingSince <- as.Date(yelpElite$YelpingSince,"%Y-%m-%d")

# Years taken to become Elite
yelpElite$TimeToElite = (yelpElite$FirstEliteYear - as.integer(format(yelpElite$YelpingSince,'%Y')))
```

## Make years as factors
```{r}
# for simplicity, drop these columns
yelpElite$YelpingSince<- NULL
yelpElite$FirstEliteYear<- NULL
yelpElite$LastEliteYear<- NULL
yelpElite$EliteYearCount <- NULL

```

```{r}
# Make training set and validation set
# set.seed (1)
# trainVector=sample(1: nrow(yelpElite), nrow(yelpElite)/2)
# train = yelpElite[trainVector, ]
# test= yelpElite[-trainVector, ]
```

```{r fig.width=12, fig.height=12}
library('corrplot')
corrplot(cor(yelpElite), # Remove non-numeric columns
         method = "number", type = "upper")
```

From the correlation plot, we could see that there is alot of high correlation indicating colinearlty in data.

```{r}
# correlation between votecool and voteuseful is 0.99. Hence drop VoteCool
yelpElite$VoteCool <- NULL

# correlation between voteuseful and votefunny is .96
yelpElite$VoteFunny <- NULL

#ComplimentCool is 0.93
yelpElite$ComplimentCool <-NULL

#ComplimentPlain 0.91
yelpElite$ComplimentPlain <-NULL

#ComplimentMore 0.91
yelpElite$ComplimentMore <-NULL

# All except ComplimentHot
yelpElite$ComplimentProfile <-NULL
yelpElite$ComplimentCute <-NULL
#train$ComplimentList <-NULL
yelpElite$ComplimentNote <-NULL
yelpElite$ComplimentCool <-NULL
yelpElite$ComplimentFunny <-NULL
yelpElite$ComplimentWriter <- NULL
```

```{r fig.width=12, fig.height=12}
library('corrplot')
corrplot(cor(yelpElite), # Remove non-numeric columns
         method = "number", type = "upper")
```

```{r}
# diagnostic plot
lm.yelp <- lm(TimeToElite~., data=yelpElite)
plot(lm.yelp)
```

```{r}
yelpEliteln <-yelpElite

yelpEliteln$ReviewCount <- log(yelpEliteln$ReviewCount)
yelpEliteln$FriendsCount <- log(yelpEliteln$FriendsCount)
yelpEliteln$Fans <- log(yelpEliteln$Fans)
yelpEliteln$VoteUseful <- log(yelpEliteln$VoteUseful)
yelpEliteln$ComplimentList <- log(yelpEliteln$ComplimentList)
yelpEliteln$ComplimentPhotos <- log(yelpEliteln$ComplimentPhotos)
yelpEliteln$ComplimentHot <- log(yelpEliteln$ComplimentHot)


# Remove infinite values

yelpEliteln<- yelpEliteln[is.finite(yelpEliteln$ReviewCount)
                          &is.finite(yelpEliteln$FriendsCount)
                          &is.finite(yelpEliteln$Fans)
                          &is.finite(yelpEliteln$VoteUseful)
                          & is.finite(yelpEliteln$ComplimentList)
                          & is.finite(yelpEliteln$ComplimentHot)
                          & is.finite(yelpEliteln$ComplimentPhotos)
                          , ]
```

```{r}
# diagnostic plot
lm.yelp <- lm(TimeToElite~. ,data=yelpEliteln)
plot(lm.yelp)
```


```{r}
# Now that the data set is clean and linear, We divide it into training and validation set
set.seed (1)
trainVector=sample(1: nrow(yelpEliteln), nrow(yelpEliteln)/2)
train = yelpEliteln[trainVector, ]
test= yelpEliteln[-trainVector, ]
```

Create the tree 
```{r}
library(tree)
tree.train = tree(TimeToElite ~ .,  data= train)
plot(tree.train)
text(tree.train ,pretty =0)
```


Do Cross-validation
```{r}
cv.yelp =cv.tree(tree.train )
plot(cv.yelp$size ,cv.yelp$dev ,type='b',
     xlab = "Number of Terminal Nodes",
     ylab = "Error Rate",
     main = "CV Error Rate Vs Number of Terminal Nodes")
```

Predict MSE
```{r}
# PERFORMING TEST PREDICTIONS ON UNPRUNED TREE
yhat=predict(tree.train, newdata=test)
timeToEliteTest.test= test[,"TimeToElite"]
plot(yhat ,timeToEliteTest.test)
abline (0,1)
testMSE = mean((yhat-timeToEliteTest.test)^2)
testMSE
sqrt(testMSE)
```

Perform bagging

```{r}
library(randomForest)
set.seed(1)
bag.yelp=randomForest(TimeToElite ~., data = train, mtry=8, importance =TRUE)
bag.yelp

# PERFORMING TEST PREDICTIONS ON BAGGED TREE
yhat.bag=predict(bag.yelp, newdata=test)

plot(yhat.bag ,timeToEliteTest.test)
abline (0,1)
testMSE = mean((yhat.bag-timeToEliteTest.test)^2)
testMSE
sqrt(testMSE)
```

Bagging didnt improve anything

Lets do Random Forest
```{r}
# PERFORMING RANDOM FOREST
set.seed(1)
rf.yelp =randomForest(TimeToElite ~., data=train, importance =TRUE)
rf.yelp

# PERFORMING TEST PREDICTIONS ON RANDOM FOREST
yhat.rf = predict(rf.yelp, newdata=test)
testMSE = mean((yhat.rf - timeToEliteTest.test)^2)
testMSE
sqrt(testMSE)

importance(rf.yelp)
varImpPlot(rf.yelp)
```

Perform BOOSTING

```{r}
library(gbm)
set.seed(1)
boost.yelp=gbm(TimeToElite~.,data = train, distribution="gaussian",
               n.trees =5000, interaction.depth = 4)
# Summary draws the relative influence plot and relative influence statistics
summary(boost.yelp)

# PERFORMING TEST PREDICTIONS ON BOOSTING
yhat.boost=predict(boost.yelp, newdata=test, n.trees =5000)
testMSE = mean((yhat.boost - timeToEliteTest.test)^2)
testMSE
sqrt(testMSE)
```